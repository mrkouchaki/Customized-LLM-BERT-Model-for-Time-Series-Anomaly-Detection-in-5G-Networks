# Customized LLM-BERT Model for Time-Series Anomaly Detection in 5G Networks

This repository is related to my Customized LLM-BERT Model, designed for anomaly detection in time-series data from 5G networks. This project represents a novel approach to applying BERT, a Transformer-based Large Language Model (LLM), to the domain of wireless communication, specifically for analyzing In-phase and Quadrature (IQ) data streams.

## üìú Overview

With the evolution of 5G networks, real-time monitoring and anomaly detection have become crucial for maintaining network reliability and performance. This project introduces a cutting-edge method by adapting BERT, a well-known Transformer-based LLM, to detect anomalies in time-series data from 5G networks.

### Key Innovations:
- **Customized Time-Series Embedding:** I have developed a customized architecture to handle time-series data by developing a specialized embedding layer that transforms the 2D sample data into a higher-dimensional space compatible with BERT‚Äôs input requirements.
- **Transformer Adaptation for Time-Series:** By treating sequences of samples as analogous to sequences of tokens in a sentence, we‚Äôve successfully applied the Transformer model to time-series data, enabling the detection of subtle anomalies over long sequences.
- **Anomaly Detection via Reconstruction Error:** The model identifies anomalies by calculating the reconstruction error between the original and reconstructed IQ data, allowing for precise detection of irregularities in the network‚Äôs performance.
- **Cross-Domain Application of LLMs:** Leveraging the power of BERT and Transformers, traditionally used in NLP, we‚Äôve demonstrated their versatility in processing and analyzing time-series data in the telecommunications domain.

## üß† Why Use BERT, LLMs, and Transformers?

### BERT (Bidirectional Encoder Representations from Transformers)

BERT is a Transformer-based model that has redefined NLP tasks by introducing bidirectional context understanding. Here‚Äôs why BERT is ideal for our model:

- **Bidirectional Contextual Understanding:** BERT allows the model to consider the full context of an IQ data sequence, making it highly effective at detecting anomalies that are dependent on patterns across multiple time steps.
- **Sequence Embeddings:** By embedding samples into a high-dimensional space, BERT enables the model to capture complex temporal dependencies, which are crucial for accurate anomaly detection.

### Large Language Models (LLMs)

LLMs like BERT are pre-trained on extensive datasets, enabling them to recognize patterns and generate coherent outputs. Applying LLMs to time-series data represents an innovative leap:

- **Advanced Pattern Recognition:** The ability of LLMs to understand and generate sequences is directly applicable to time-series data, where each IQ sequence can be treated like a sentence in a language model.
- **Transfer Learning:** Adapting a pre-trained LLM to new domains like time-series data in telecommunications demonstrates the power of transfer learning, where the knowledge from one domain accelerates learning in another.

### Transformers

Transformers, the backbone of models like BERT, excel in sequence-to-sequence tasks and offer unique advantages:

- **Self-Attention Mechanism:** This mechanism allows the model to weigh the importance of each time step in relation to others, capturing dependencies across the entire sequence, which is crucial for analyzing long IQ data sequences.
- **Scalability and Flexibility:** Transformers are scalable and can handle varying sequence lengths, making them ideal for processing the extensive and complex sequences typical in 5G networks.

## üõ†Ô∏è Implementation Details

### Data Preparation

The time-series data comprises IQ samples stored in binary format. The `TimeSeriesDataset` class efficiently loads and prepares this data, splitting it into sequences that are then fed into the model.

### Model Architecture

The `TimeSeriesBERT` model is the core of this project, featuring several key components:

- **Customized Time-Series Embedding Layer:** Converts the two-dimensional IQ data into a format suitable for BERT, allowing the model to process time-series data effectively.
- **BERT Encoder:** Processes the embedded sequences, leveraging its bidirectional context understanding to analyze the sequence holistically.
- **Reconstruction Layer:** Maps the encoded sequence back to the original IQ data space, enabling the calculation of reconstruction errors, which are then used to detect anomalies.

### Training Process

- **Device Configuration:** The model is designed to run on either GPU or CPU, automatically adjusting to the available hardware.
- **Training Loop:** The model is trained to minimize the Mean Squared Error (MSE) between the original and reconstructed IQ sequences. The `Adam` optimizer is chosen for its efficiency in handling large-scale models.
- **Evaluation:** During evaluation, reconstruction errors are computed for each sequence, with anomalies detected based on these errors exceeding a predefined threshold.

### Anomaly Detection

- **Threshold Setting:** Anomalies are detected by setting a threshold at the 95th percentile of the reconstruction error distribution.
- **Result Visualization:** Detailed visualizations are provided, allowing for the analysis of both normal and anomalous sequences, offering insights into the model‚Äôs performance.

## üöÄ Getting Started

### Prerequisites

- Python 3.x
- PyTorch
- Transformers (Hugging Face)
- Matplotlib
- NumPy
- Pandas

### Installation

1. Clone this repository:
    ```bash
    git clone https://github.com/yourusername/Customized-LLM-BERT-5G-Anomaly-Detection.git
    cd Customized-LLM-BERT-5G-Anomaly-Detection
    ```

2. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

3. Prepare your dataset in binary format and place it in the appropriate directory.

### Running the Model

1. **Train the Model:**
    ```bash
    python train.py --data_path /path/to/your/training/data --sequence_length 100 --batch_size 256 --epochs 20
    ```

2. **Evaluate and Detect Anomalies:**
    ```bash
    python evaluate.py --data_path /path/to/your/test/data --model_path /path/to/saved/model --threshold 0.95
    ```

3. **Visualize Results:**
    - Use the provided visualization scripts to generate plots and gain insights into the model‚Äôs performance.

## üìä Results and Analysis

### Training and Evaluation Loss
Training is monitored through loss curves, providing insights into the model‚Äôs learning progress and ensuring no overfitting.

### Anomaly Detection
Anomalies are detected by analyzing reconstruction errors, with visualizations highlighting the differences between normal and anomalous sequences.

### Visualization
Detailed visualizations are provided, including reconstruction errors, loss curves, and comparisons between original and reconstructed sequences.

## üìù Reference

This project showcases the innovative application of BERT and Transformers for time-series anomaly detection in 5G networks. It highlights the power of cross-domain applications of LLMs, bringing advanced NLP techniques to the field of signal processing.

## üì´ Contact

If you have any questions or suggestions, feel free to reach out to me:

- **Email:** [mrkouchakii@gmail.com](mailto:mrkouchakii@gmail.com)
- **LinkedIn:** [linkedin.com/in/mohammadreza-kouchaki](https://www.linkedin.com/in/mohammadreza-kouchaki/)
